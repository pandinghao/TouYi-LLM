import gradio as gr
import re
# from TouyiModel import generate_response
from transformers import AutoTokenizer
import torch

import sys
sys.path.append("./")
from utils import ModelUtils
from finetune import make_supervised_data_module,preprocess

model_name_or_path = "Qwen_model/Qwen/Qwen-7B"      # Qwenæ¨¡å‹æƒé‡è·¯å¾„
adapter_name_or_path = "/root/autodl-tmp/output_qwen_stage2_1030/checkpoint-21926"    # sftåadapteræƒé‡è·¯å¾„
load_in_4bit = False
device = 'cuda:0'
model = ModelUtils.load_model(
    model_name_or_path,
    load_in_4bit=load_in_4bit,
    adapter_name_or_path=adapter_name_or_path
).eval()

tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    trust_remote_code=True,
    padding_side='left',
    # llamaä¸æ”¯æŒfast
    use_fast=False if model.config.model_type == 'llama' else True
)
if tokenizer.__class__.__name__ == 'QWenTokenizer':
    tokenizer.pad_token_id = tokenizer.eod_id
    tokenizer.bos_token_id = tokenizer.eod_id
    tokenizer.eos_token_id = tokenizer.eod_id
#

TOUYI_DES = """
<font size=4>âœŒï¸<b style="color: red">è‡ªä¸»æ¡†æ¶æ­å»ºï¼š </b>å®Œå…¨è„±ç¦»æµè¤å·¥å…·ï¼ŒåŸºäºTrainerç‹¬ç«‹å®ç°è®­ç»ƒè¿‡ç¨‹</font>
<br>
<font size=4>ğŸ™Œ<b style="color: red">å¾®è°ƒç®—æ³•è°ƒæ•´ï¼š </b>é‡‡ç”¨çš„qloraå¾®è°ƒæ–¹æ³•ä¸­ï¼Œé™¤äº†å…¨è¿æ¥å±‚å¤–ï¼Œè¿˜æ·»åŠ äº†LN headå‚æ•°</font>
<br>
<font size=4>ğŸ‘†<b style="color: red">è®­ç»ƒéªŒè¯å¹¶å‘ï¼š </b>é‡å†™Trainerä¸­EvalPredictionæ–¹æ³•ï¼Œå®ç°è¾¹è®­ç»ƒè¾¹éªŒè¯æŒ‡æ ‡éªŒè¯</font>
<br>
<font size=4>ğŸ‘<b style="color: red">è®­ç»ƒæ–¹æ³•è®¾è®¡ï¼š </b>ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå…ˆä»»åŠ¡åå¯¹è¯ï¼Œé˜²æ­¢å¯¹è¯æ•°æ®è¿‡æ‹Ÿåˆ</font>
<br>
<font size=4>â­<b style="color: red">BM25æ£€ç´¢å¢å¼ºï¼š </b>é’ˆå¯¹ä»»åŠ¡è¿›è¡Œæ£€ç´¢å¾—åˆ°demonstrationï¼ŒåŠ å…¥è®­ç»ƒçš„æ ·ä¾‹ä¸­</font>
<br>
<br>
<div style="font-size: 13pt;">
<font size=4>ğŸš©<b style="color: red">TouYié¡¹ç›®åœ°å€ï¼š </b>æˆ‘ä»¬çš„é¡¹ç›®å·²ç»åœ¨<img src="https://pic.imgdb.cn/item/651401a4c458853aef46f7f5.png" style="width: 13pt; display: inline-block;"> <a href="https://github.com/pandinghao/TouYi-LLM">https://github.com/pandinghao/TouYi-LLM</a>ä¸Šå¼€æº
<br>
"""


NEWLINES = """
<br>

"""

custom_css = """
#banner-image {
    margin-left: auto;
    margin-right: auto; 
}

#loss-image {
    margin-left: auto;
    margin-right: auto; 
}
"""

# æ¸…ç©ºå¹¶æäº¤æ–‡æœ¬æ¡†
def clear_and_save_textbox(message: str):
    return '', message

# å¤„ç†ç¤ºä¾‹é—®é¢˜çš„å‡½æ•°
def process_example(message: str):
    generator = generate(message, [], 500, 0.10, 0.9)
    return '', generator

# retryæŒ‰é”®å›é€€å¯¹è¯è®°å½•
def delete_prev_fn(history):
    try:
        message, _ = history.pop()
    except IndexError:
        message = ''
    return history, message or ''

# æ£€æŸ¥è¾“å…¥æ˜¯å¦è¿è§„ï¼Œå³è‹¥ä¸åŒ…å«ä¸­æ–‡å’Œè‹±æ–‡åˆ™åˆ¤ä¸ºè¿è§„å­—ç¬¦
def check_ch_en(text):
    ch_pat = re.compile(u'[\u4e00-\u9fa5]') 
    en_pat = re.compile(r'[a-zA-Z]')
    has_ch = ch_pat.search(text)
    has_en = en_pat.search(text)
    if has_ch and has_en:
        return True 
    elif has_ch:
        return True 
    elif has_en:
        return True 
    else:
        return False


# åˆ é™¤å†å²æ¶ˆæ¯ä¸­çš„ç³»ç»Ÿæç¤ºè¯­å¥ï¼Œé˜²æ­¢å½±å“å¯¹è¯æ€§èƒ½
def Delete_Specified_String(history):
    # åˆ›å»ºä¸€ä¸ªæ–°çš„äºŒç»´åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ä¸åŒ…å«'ä½ çš„è¾“å…¥æ— æ•ˆ'å›ç­”çš„é—®ç­”å¯¹
    filtered_history = []
    # éå†åŸå§‹äºŒç»´åˆ—è¡¨
    for pair in history:
        if pair[1] != 'æ‚¨çš„è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ï¼Œè°¢è°¢ï¼':
            filtered_history.append(pair)
    # ç°åœ¨filtered_historyä¸­åŒ…å«ä¸åŒ…å«'ä½ çš„è¾“å…¥æ— æ•ˆ'å›ç­”çš„é—®ç­”å¯¹
    return filtered_history


# åå¤„ç†ï¼Œåˆ é™¤ç”Ÿæˆä¸­çš„é‡å¤ç”Ÿæˆéƒ¨åˆ†
def remove_continuous_duplicate_sentences(text):
    
    sentences = re.split(r'([ã€‚,ï¼›ï¼Œ\n])', text)
    
    # åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„æ–‡æœ¬åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å»é™¤è¿ç»­é‡å¤å¥å­åçš„ç»“æœ
    new_sentences = [sentences[0]]  # å°†ç¬¬ä¸€ä¸ªå¥å­æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    
    # éå†å¥å­åˆ—è¡¨ï¼Œä»…æ·»åŠ ä¸ä¸å‰ä¸€ä¸ªå¥å­ç›¸åŒçš„å¥å­
    for i in range(2, len(sentences), 2):
        if sentences[i] != sentences[i - 2]:
            new_sentences.append(sentences[i - 1] + sentences[i])
    
    # é‡æ–°æ„å»ºæ–‡æœ¬ï¼Œä½¿ç”¨åŸå§‹æ ‡ç‚¹ç¬¦å·è¿æ¥å¥å­
    new_text = ''.join(new_sentences)
    
    return new_text



# ç”Ÿæˆå‡½æ•°ï¼Œè°ƒç”¨åç«¯æ¨¡å‹ç”Ÿæˆå›å¤
def generate(
    message: str,
    history,
    max_new_tokens: int,
    temperature: float,
    top_p: float
):
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦
    if not check_ch_en(message):
        generator = "æ‚¨çš„è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ï¼Œè°¢è°¢ï¼"
        return  history+[(message.replace('\n', '\n\n'), generator)]
    # ç”Ÿæˆè¶…å‚é…ç½®n
    repetition_penalty = 1.0
    history_max_len = 1000  # æ¨¡å‹è®°å¿†çš„æœ€å¤§tokené•¿åº¦
    conversation = list()
    for user_his, assist_his in history:
        conversation.append({"from": "user", "value": user_his})
        conversation.append({"from": "assistant", "value": assist_his})
    conversation.append({"from": "user", "value": message})
    #print("conversation:")
    #print(conversation)
    data_dict = preprocess([conversation], tokenizer, 1024, test_flag = False,multiturn_flag=True,history_max_len=history_max_len)    
    input_ids = data_dict["input_ids"]
    #print(input_ids)
    input_ids = torch.tensor(input_ids, dtype=torch.int).to(device=device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,
            top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,
            eos_token_id=tokenizer.eos_token_id
        )
    #print("outputs")
    #print(outputs)
    outputs = outputs.tolist()[0][len(input_ids[0]):]
    response = tokenizer.decode(outputs,skip_special_tokens = True)
    response = remove_continuous_duplicate_sentences(response)
    #print(response)
    history.append((message,response))
    #print(history)
    # æ£€æŸ¥historyä¸­æ˜¯å¦åŒ…å«äº†åå¤„ç†çš„ä¿¡æ¯
    history = Delete_Specified_String(history)
    # generator = generate_response(message, history, max_new_tokens, temperature, top_p)
    #history.append((message,generator))
    return history




with gr.Blocks(css = custom_css) as demo:
    with gr.Row():
        with gr.Column():
            gr.Markdown("# Touyi Sparse Finetuned Demo")
            gr.Markdown(TOUYI_DES)
            gr.Markdown(NEWLINES)
            gr.Markdown("# NERã€REä»»åŠ¡F1è¯„åˆ†å±•ç¤ºï¼š")
            gr.Image("chat/F1.png", elem_id="banner-image", show_label=False, container=False)
            gr.Markdown(NEWLINES)
            gr.Markdown("# è®­ç»ƒè¿‡ç¨‹Losså±•ç¤ºï¼š")
            gr.Image("chat/loss.png", elem_id="loss-image", show_label=False, container=False)
        with gr.Column():
            gr.Markdown("""### Touyi Sparse Finetuned Demo""")

            with gr.Group():
                chatbot = gr.Chatbot(
                    label = 'Chatbot',
                    bubble_full_width=False,
                    avatar_images=("chat/patient.jpg", "chat/doctor.jpg"),
                    height=600    
                )

                textbox = gr.Textbox(
                    container=False,
                    show_label=False,
                    placeholder="å¤´ä¸€æ— æ•Œ",
                    lines=6
                )


            with gr.Row():
                retry_button = gr.Button('ğŸ”„  Retry', variant='secondary')
                undo_button = gr.Button('â†©ï¸ Undo', variant='secondary')
                clear_button = gr.Button('ğŸ—‘ï¸  Clean', variant='secondary')
                submit_button = gr.Button('ğŸš© Submit',variant='primary')



            with gr.Accordion(label = 'RE Example', open = False):
                gr.Examples(
                examples=[
                    'å¯¹ä¸‹é¢è¯­å¥è¿›è¡Œå…³ç³»æŠ½å–\n"æˆ‘å›½æ–°ä¸­å›½æˆç«‹å‰æ¯å¹´çº¦100ä¸‡æ–°ç”Ÿå„¿æ­»äºç ´ä¼¤é£ï¼Œå»ºå›½åå‘ç—…ç‡å’Œæ­»äº¡ç‡æ˜¾è‘—ä¸‹é™ï¼Œä½†åœ¨è¾¹è¿œå†œæ‘ã€å±±åŒºåŠç§è‡ªæ¥ç”Ÿè€…æ–°ç”Ÿå„¿ç ´ä¼¤é£ä»ä¸ç½•è§ã€‚ (å››)æŠ—ç”Ÿç´ é’éœ‰ç´ : èƒ½æ€ç­ç ´ä¼¤é£æ¢­èŒï¼Œ10ä¸‡~20ä¸‡u/ (kgÂ·d) ï¼Œæ¯å¤©åˆ†2æ¬¡ï¼Œç–—ç¨‹10å¤©å·¦å³ã€‚\nå…³ç³»ç±»å‹: é˜¶æ®µï¼Œæ”¾å°„æ²»ç–—ï¼Œè¯ç‰©æ²»ç–—ï¼Œç­›æŸ¥ï¼Œå®éªŒå®¤æ£€æŸ¥ï¼Œå¤šå‘å­£èŠ‚ï¼Œç—…ç†åˆ†å‹ï¼Œå¤šå‘åœ°åŒºï¼Œé—ä¼ å› ç´ ï¼Œå¹¶å‘ç—‡ï¼ŒåŒä¹‰è¯ï¼ŒåŒ–ç–—ï¼Œå¤šå‘ç¾¤ä½“ï¼Œè½¬ç§»éƒ¨ä½ï¼Œç—…å²ï¼Œé£é™©è¯„ä¼°å› ç´ ï¼Œå½±åƒå­¦æ£€æŸ¥ï¼Œç»„ç»‡å­¦æ£€æŸ¥ï¼Œé¢„åç”Ÿå­˜ç‡ï¼Œä¾µåŠå‘¨å›´ç»„ç»‡è½¬ç§»çš„ç—‡çŠ¶ï¼Œç—…ç†ç”Ÿç†ï¼Œå°±è¯Šç§‘å®¤ï¼Œé¢„åçŠ¶å†µï¼Œå¤–ä¾µéƒ¨ä½ï¼Œé«˜å±å› ç´ ï¼Œå†…çª¥é•œæ£€æŸ¥ï¼Œå‘ç—…æœºåˆ¶ï¼Œç›¸å…³(å¯¼è‡´)ï¼Œå‘ç—…ç‡ï¼Œç—…å› ï¼Œæ‰‹æœ¯æ²»ç–—ï¼Œæ²»ç–—åç—‡çŠ¶ï¼Œé‰´åˆ«è¯Šæ–­ï¼Œé¢„é˜²ï¼Œæ­»äº¡ç‡ï¼Œä¼ æ’­é€”å¾„ï¼Œå‘ç—…æ€§åˆ«å€¾å‘ï¼Œè¾…åŠ©æ²»ç–—ï¼Œç›¸å…³(ç—‡çŠ¶)ï¼Œå‘ç—…å¹´é¾„ï¼Œå‘ç—…éƒ¨ä½ï¼Œä¸´åºŠè¡¨ç°ï¼Œè¾…åŠ©æ£€æŸ¥ï¼Œç›¸å…³(è½¬åŒ–)"',
                ],
                inputs=textbox,
                outputs=[textbox, chatbot],
                fn=process_example,
                cache_examples=False,
                label=""
                )
            
            with gr.Accordion(label = 'NER Example', open = False):
                gr.Examples(
                examples=[
                    'åœ¨ä¸‹è¿°æ–‡æœ¬ä¸­æ ‡è®°å‡ºåŒ»å­¦å®ä½“:\nSARSç—…ç†ç”Ÿç†è¿‡ç¨‹çš„å…³é”®æ˜¯å…¨èº«ç‚ç—‡ååº”ç»¼åˆå¾ (SIRS)çš„ä¸æ–­æ”¾å¤§ï¼Œå‘ç”Ÿçº§è”ååº”(cascade)ï¼Œå¯¼è‡´â€œç»†èƒå› å­é£æš´â€å’Œâ€œç‚ç—‡ä»‹è´¨ç€‘å¸ƒâ€ï¼›ä¹Ÿå¯èƒ½å‘ç”Ÿâ€œè‚ é“ç»†èŒç§»ä½â€å’Œâ€œè‚ æºæ€§å†…æ¯’ç´ è¡€ç—‡â€ï¼Œè¿›è€Œå‘ç”Ÿæ„ŸæŸ“æ€§ä¼‘å…‹å’Œç»„ç»‡å™¨å®˜æŸä¼¤ï¼Œå¯¼è‡´MODSå’ŒMOFã€‚\nå¯è¯†åˆ«çš„å®ä½“ç±»å‹æœ‰: è¯ç‰©ï¼Œå¾®ç”Ÿç‰©ç±»ï¼ŒåŒ»ç–—è®¾å¤‡ï¼Œèº«ä½“ï¼ŒåŒ»å­¦æ£€éªŒé¡¹ç›®ï¼Œä¸´åºŠè¡¨ç°ï¼ŒåŒ»ç–—ç¨‹åºï¼Œç–¾ç—…ï¼Œç§‘å®¤',
                ],
                inputs=textbox,
                outputs=[textbox, chatbot],
                fn=process_example,
                cache_examples=False,
                label=""
                )



            max_new_tokens = gr.Slider(
                label='Max new tokens',
                minimum=1,
                maximum=800,
                step=1,
                value=500,
                interactive=True,
            )
            temperature = gr.Slider(
                label='Temperature',
                minimum=0,
                maximum=1,
                step=0.05,
                value=0.20,
                interactive=True,
            )
            top_p = gr.Slider(
                label='Top-p (nucleus sampling)',
                minimum=0,
                maximum=1.0,
                step=0.1,
                value=0.9,
                interactive=True,
            )
            history_max_len = gr.Slider(
                label='History Max Length',
                minimum=500,
                maximum=1000,
                step=1,
                value=1000,
                interactive=True,
            )
    
    
    saved_input = gr.State()

    textbox.submit(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    button_event_preprocess = submit_button.click(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    retry_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    undo_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=lambda x: x,
        inputs=[saved_input],
        outputs=textbox,
        api_name=False,
        queue=False,
    )

    clear_button.click(
        fn=lambda: ([], ''),
        outputs=[chatbot, saved_input],
        queue=False,
        api_name=False,
    )

if __name__ == '__main__':
    demo.queue().launch(server_name="0.0.0.0", server_port=6006)
