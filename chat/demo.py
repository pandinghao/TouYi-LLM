import gradio as gr
import re
# from TouyiModel import generate_response
from transformers import AutoTokenizer
import torch

import sys
sys.path.append("./")
from utils import ModelUtils
from finetune import make_supervised_data_module,preprocess

model_name_or_path = "Qwen_model/Qwen/Qwen-7B"      # Qwenæ¨¡å‹æƒé‡è·¯å¾„
adapter_name_or_path = "/root/autodl-tmp/output_qwen_stage2_1030"     # sftåadapteræƒé‡è·¯å¾„
load_in_4bit = False
device = 'cuda:0'
model = ModelUtils.load_model(
    model_name_or_path,
    load_in_4bit=load_in_4bit,
    adapter_name_or_path=adapter_name_or_path
).eval()

tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    trust_remote_code=True,
    padding_side='left',
    # llamaä¸æ”¯æŒfast
    use_fast=False if model.config.model_type == 'llama' else True
)
if tokenizer.__class__.__name__ == 'QWenTokenizer':
    tokenizer.pad_token_id = tokenizer.eod_id
    tokenizer.bos_token_id = tokenizer.eod_id
    tokenizer.eos_token_id = tokenizer.eod_id
#
TOUYI_DES = """
<br>
<div align="center" style="font-size: 13pt;">
&nbsp;&nbsp;<img src="https://pic.imgdb.cn/item/651401a4c458853aef46f7f5.png" style="width: 13pt; display: inline-block;"> <a href="https://github.com/pandinghao/TouYi-LLM">Github</a>
<br>
"""


custom_css = """
#banner-image {
    margin-left: auto;
    margin-right: auto;
    width: 70px;
    height: 80px
}
"""

# æ¸…ç©ºå¹¶æäº¤æ–‡æœ¬æ¡†
def clear_and_save_textbox(message: str):
    return '', message

# å¤„ç†ç¤ºä¾‹é—®é¢˜çš„å‡½æ•°
def process_example(message: str):
    generator = generate(message, [], 500, 0.10, 0.9)
    return '', generator

# retryæŒ‰é”®å›é€€å¯¹è¯è®°å½•
def delete_prev_fn(history):
    try:
        message, _ = history.pop()
    except IndexError:
        message = ''
    return history, message or ''

# æ£€æŸ¥è¾“å…¥æ˜¯å¦è¿è§„ï¼Œå³è‹¥ä¸åŒ…å«ä¸­æ–‡å’Œè‹±æ–‡åˆ™åˆ¤ä¸ºè¿è§„å­—ç¬¦
def check_ch_en(text):
    ch_pat = re.compile(u'[\u4e00-\u9fa5]') 
    en_pat = re.compile(r'[a-zA-Z]')
    has_ch = ch_pat.search(text)
    has_en = en_pat.search(text)
    if has_ch and has_en:
        return True 
    elif has_ch:
        return True 
    elif has_en:
        return True 
    else:
        return False


# åˆ é™¤å†å²æ¶ˆæ¯ä¸­çš„ç³»ç»Ÿæç¤ºè¯­å¥ï¼Œé˜²æ­¢å½±å“å¯¹è¯æ€§èƒ½
def Delete_Specified_String(history):
    # åˆ›å»ºä¸€ä¸ªæ–°çš„äºŒç»´åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨ä¸åŒ…å«'ä½ çš„è¾“å…¥æ— æ•ˆ'å›ç­”çš„é—®ç­”å¯¹
    filtered_history = []
    # éå†åŸå§‹äºŒç»´åˆ—è¡¨
    for pair in history:
        if pair[1] != 'æ‚¨çš„è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ï¼Œè°¢è°¢ï¼':
            filtered_history.append(pair)
    # ç°åœ¨filtered_historyä¸­åŒ…å«ä¸åŒ…å«'ä½ çš„è¾“å…¥æ— æ•ˆ'å›ç­”çš„é—®ç­”å¯¹
    return filtered_history


# åå¤„ç†ï¼Œåˆ é™¤ç”Ÿæˆä¸­çš„é‡å¤ç”Ÿæˆéƒ¨åˆ†
def remove_continuous_duplicate_sentences(text):
    
    sentences = re.split(r'([ã€‚,ï¼›ï¼Œ\n])', text)
    
    # åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„æ–‡æœ¬åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å»é™¤è¿ç»­é‡å¤å¥å­åçš„ç»“æœ
    new_sentences = [sentences[0]]  # å°†ç¬¬ä¸€ä¸ªå¥å­æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    
    # éå†å¥å­åˆ—è¡¨ï¼Œä»…æ·»åŠ ä¸ä¸å‰ä¸€ä¸ªå¥å­ç›¸åŒçš„å¥å­
    for i in range(2, len(sentences), 2):
        if sentences[i] != sentences[i - 2]:
            new_sentences.append(sentences[i - 1] + sentences[i])
    
    # é‡æ–°æ„å»ºæ–‡æœ¬ï¼Œä½¿ç”¨åŸå§‹æ ‡ç‚¹ç¬¦å·è¿æ¥å¥å­
    new_text = ''.join(new_sentences)
    
    return new_text



# ç”Ÿæˆå‡½æ•°ï¼Œè°ƒç”¨åç«¯æ¨¡å‹ç”Ÿæˆå›å¤
def generate(
    message: str,
    history,
    max_new_tokens: int,
    temperature: float,
    top_p: float
):
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦
    if not check_ch_en(message):
        generator = "æ‚¨çš„è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ï¼Œè°¢è°¢ï¼"
        return  history+[(message.replace('\n', '\n\n'), generator)]
    # ç”Ÿæˆè¶…å‚é…ç½®n
    repetition_penalty = 1.0
    history_max_len = 1000  # æ¨¡å‹è®°å¿†çš„æœ€å¤§tokené•¿åº¦
    conversation = list()
    for user_his, assist_his in history:
        conversation.append({"from": "user", "value": user_his})
        conversation.append({"from": "assistant", "value": assist_his})
    conversation.append({"from": "user", "value": message})
    #print("conversation:")
    #print(conversation)
    data_dict = preprocess([conversation], tokenizer, 1024, test_flag = False,multiturn_flag=True,history_max_len=history_max_len)    
    input_ids = data_dict["input_ids"]
    #print(input_ids)
    input_ids = torch.tensor(input_ids, dtype=torch.int).to(device=device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,
            top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,
            eos_token_id=tokenizer.eos_token_id
        )
    #print("outputs")
    #print(outputs)
    outputs = outputs.tolist()[0][len(input_ids[0]):]
    response = tokenizer.decode(outputs,skip_special_tokens = True)
    generator = remove_continuous_duplicate_sentences(response)
    #print(response)
    history.append((message,response))
    #print(history)
    # æ£€æŸ¥historyä¸­æ˜¯å¦åŒ…å«äº†åå¤„ç†çš„ä¿¡æ¯
    history = Delete_Specified_String(history)
    # generator = generate_response(message, history, max_new_tokens, temperature, top_p)
    #history.append((message,generator))
    return history




with gr.Blocks(css = custom_css) as demo:
    with gr.Row():
        with gr.Column():
            gr.Markdown("# Touyi Sparse Finetuned Demo")
            gr.Markdown(TOUYI_DES)
            gr.Image("chat/test.jpg", elem_id="banner-image", show_label=False, container=False)
        with gr.Column():
            gr.Markdown("""### Touyi Sparse Finetuned Demo""")

            with gr.Group():
                chatbot = gr.Chatbot(
                    label = 'Chatbot',
                    bubble_full_width=False    
                )

                textbox = gr.Textbox(
                    container=False,
                    show_label=False,
                    placeholder="å¤´ä¸€æ— æ•Œ",
                    lines=6
                )


            with gr.Row():
                retry_button = gr.Button('ğŸ”„  Retry', variant='secondary')
                undo_button = gr.Button('â†©ï¸ Undo', variant='secondary')
                clear_button = gr.Button('ğŸ—‘ï¸  Clear', variant='secondary')
                submit_button = gr.Button('ğŸš© Submit',variant='primary')



            gr.Examples(
            examples=[
                'æœ€è¿‘è‚šå­æ€»æ˜¯éšéšä½œç—›ï¼Œæ„Ÿè§‰èƒ€èƒ€çš„ï¼Œåƒä¸‹å»çš„ä¸œè¥¿éƒ½æ²¡æ³•å¸æ”¶,è¯·é—®æ˜¯ä»€ä¹ˆå›äº‹ï¼Ÿ',
                'What is the best treatment for sleep problems?'
            ],
            inputs=textbox,
            outputs=textbox,
            fn=process_example,
            cache_examples=False,
            label='Question Answering'
            )



            max_new_tokens = gr.Slider(
                label='Max new tokens',
                minimum=1,
                maximum=800,
                step=1,
                value=500,
                interactive=True,
            )
            temperature = gr.Slider(
                label='Temperature',
                minimum=0,
                maximum=1,
                step=0.05,
                value=0.10,
                interactive=True,
            )
            top_p = gr.Slider(
                label='Top-p (nucleus sampling)',
                minimum=0,
                maximum=1.0,
                step=0.1,
                value=0.9,
                interactive=True,
            )
            history_max_len = gr.Slider(
                label='History Max Length',
                minimum=500,
                maximum=1000,
                step=1,
                value=1000,
                interactive=True,
            )
    
    
    saved_input = gr.State()

    textbox.submit(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    button_event_preprocess = submit_button.click(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    retry_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            max_new_tokens,
            temperature,
            top_p,
        ],
        outputs=chatbot,
        api_name=False,
    )

    undo_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=lambda x: x,
        inputs=[saved_input],
        outputs=textbox,
        api_name=False,
        queue=False,
    )

    clear_button.click(
        fn=lambda: ([], ''),
        outputs=[chatbot, saved_input],
        queue=False,
        api_name=False,
    )



demo.queue().launch(server_name="0.0.0.0", server_port=6006)
